{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bermanlabemory/gait_signatures/blob/main/Gait_Signatures_Script_2_Extract_Internal_Parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onLC9Msr6_RX"
      },
      "source": [
        "## This script accesses the trained models in their respective folders from 'Gait Signatures Script 1: Train Model Architectures.ipynb', extracts externally-driven and self-driven Hs and Cs and plots respective predicted kinematics.\n",
        "\n",
        "**notes:** \n",
        "1. Each model's parameters (Hs and Cs) are extracted and used to develop the signatures\n",
        "\n",
        "**Created by**: Taniel Winner\n",
        "\n",
        "**Date**: 07/*18*/22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLBAyLXe7udu"
      },
      "source": [
        "**Step 0**: Mount (connect to) your google drive folder where you want to save the simulation results and model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iuzkec-P1z7g",
        "outputId": "dd1706b7-d93b-483d-e7c8-f4dd8a06c0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfiYeSIR702R",
        "outputId": "d11e5c7e-28d6-4efc-9431-530033dc77ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.8.16\n"
          ]
        }
      ],
      "source": [
        "# check python version \n",
        "from platform import python_version\n",
        "\n",
        "print(python_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-gE34Gq705Z",
        "outputId": "c2eac3b9-c125-4786-cafe-816adcdb51f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "# check tensorflow version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmVki5IZ708h",
        "outputId": "a5ad0af5-95ae-47f7-ada6-58dc2dca5cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDY0A5Qa77-b"
      },
      "source": [
        "**Step 1**: Import necessary packages and functions to develop model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sjnyu-Hi70_Z"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.model_selection as model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import keras as k\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import copy\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "from scipy import interpolate\n",
        "from numpy import sin,cos,pi,array,linspace,cumsum,asarray,dot,ones\n",
        "from pylab import plot, legend, axis, show, randint, randn, std,lstsq\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "import csv\n",
        "import os\n",
        "from tqdm import tqdm "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMRQ1Zrk_5m9",
        "outputId": "cd84088f-065b-4d45-e701-7b9686eaeeac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/GaitSignature_Manuscript\n",
            "-rw------- 1 root root 7259 Apr  4  2022 fourierseries.py\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/My\\ Drive/GaitSignature_Manuscript/\n",
        "\n",
        "# Ensure fourierseries.py is in the pathway\n",
        "!ls -l fourierseries.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcev1QZsZFqh"
      },
      "source": [
        "Phaser code developed by Shai Revzen (2018)\n",
        "\n",
        "This function takes multivariate periodic time series and finds estimates of phase over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "956M3pr8_56k",
        "outputId": "77f6ec29-4652-40e0-a174-278307090509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running phaser\n"
          ]
        }
      ],
      "source": [
        "import fourierseries\n",
        "import util\n",
        "import phaser\n",
        "import dataloader\n",
        "# Preprocess data for a single subject - to be send to modeling frameworks\n",
        "def find_phase(k):\n",
        "    \"\"\"\n",
        "    Detrend and compute the phase estimate using Phaser\n",
        "    INPUT:\n",
        "      k -- dataframe\n",
        "    OUTPUT:\n",
        "      k -- dataframe\n",
        "    \"\"\"\n",
        "    #l = ['hip_flexion_l','hip_flexion_r'] # Phase variables = hip flexion angles\n",
        "    y = np.array(k)\n",
        "    print(y.shape)\n",
        "    y = util.detrend(y.T).T\n",
        "    print(y.shape)\n",
        "    phsr = phaser.Phaser(y=y)\n",
        "    k[:] = phsr.phaserEval(y)[0,:]\n",
        "    return k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R4dHUEiZMLO"
      },
      "source": [
        "Von Mises interpolation function \n",
        "This function is used to:..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEWFCHvbAYc1"
      },
      "outputs": [],
      "source": [
        "def vonMies(t,t_0, b):\n",
        "    out = np.exp(b*np.cos(t-t_0))/(2*pi*iv(0, b))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4J5BZIg8EYj"
      },
      "source": [
        "**Step 2**: Load module in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PRNNDCI71CQ"
      },
      "outputs": [],
      "source": [
        "# The path to save the models and read data from\n",
        "\n",
        "### !!! specify folder name the same as in script 1\n",
        "folder = 'Gait_signatures/'\n",
        "\n",
        "path = '/content/drive/My Drive/' + folder\n",
        "\n",
        "# Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAhc9QkW8ONR"
      },
      "source": [
        "**Step 3**: Load in data and specify variables/parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXEEUgWB8Mk0",
        "outputId": "cf79f828-15cd-4e35-ea04-7c31e0472ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original input data shape is: (108000, 6)\n",
            "input data reshaped is: (72, 1500, 6)\n"
          ]
        }
      ],
      "source": [
        "# Non-changing variables \n",
        "\n",
        "# number of trials in dataset \n",
        "trialnum = 72 # 72 total trials\n",
        "\n",
        "# number of samples in each trial\n",
        "trialsamp = 1500\n",
        "\n",
        "# number of features collected per trial\n",
        "feats = 6\n",
        "\n",
        "#Batch size - same as the number of traintrials\n",
        "batch_size = trialnum\n",
        "\n",
        "# Number of Layers\n",
        "numlayers = 1\n",
        "\n",
        "# Choose the number of iterations to train the model- if this script has been run previously enter a value greater than was \n",
        "# inputted before and rerun the script. \n",
        "finalepoch = 10000\n",
        "\n",
        "# load the input data/kinematics\n",
        "datafilepath = path + 'PareticvsNonP_RNNData.csv' #input data\n",
        "all_csvnp = np.loadtxt(datafilepath,delimiter=',').T\n",
        "\n",
        "# reshape all the input data into a tensor\n",
        "all_inputdata_s = all_csvnp.reshape(trialnum,trialsamp,feats) \n",
        "\n",
        "print('original input data shape is: '+ str(all_csvnp.shape ))\n",
        "print('input data reshaped is: '+ str(all_inputdata_s.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_A7pitQ8XS9"
      },
      "source": [
        "**Step 4**: List of model architectures and corresponding variables from script 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAN8l21V8ZeU"
      },
      "outputs": [],
      "source": [
        "# generate a list of models and corresponding parameters to test \n",
        "test_model_nodes = [512] \n",
        "seqs = [249,499] #lookback parameter\n",
        "\n",
        "# run multiple model architechtures many times to test stability of cost function outputs\n",
        "runs = 1 # stability analysis - repeat each model architecture this many times\n",
        "test_model_seq = np.repeat(seqs, runs)\n",
        "\n",
        "All_nodes = np.empty([0,1], dtype='int')\n",
        "All_seq = np.empty([0,1],dtype='int')\n",
        "All_valseg = np.empty([0,1],dtype='int')\n",
        "All_trainseg = np.empty([0,1],dtype='int')\n",
        "All_modelname = []\n",
        "All_mod_name = []\n",
        "\n",
        "count = 0; #initialize model run -- this serves as the model run ID number\n",
        "for a in test_model_nodes:\n",
        "  for b in test_model_seq:\n",
        "    if count < runs:\n",
        "      count = count + 1 \n",
        "    else: \n",
        "      count = 1 # reset counter when all runs of certain model attained\n",
        "    #if statement for valseg, trainseg based on sequence length\n",
        "    if int(b) == 249:\n",
        "      trainseg = 4\n",
        "      valseg = 2\n",
        "    elif int(b) == 499: \n",
        "      trainseg = 2\n",
        "      valseg = 1\n",
        "    elif int(b) == 749:\n",
        "      trainseg = 1\n",
        "      valseg = 1\n",
        "\n",
        "    All_nodes = np.append(All_nodes, a) \n",
        "    All_seq = np.append(All_seq, int(b))\n",
        "    All_valseg = np.append(All_valseg, valseg)\n",
        "    All_trainseg = np.append(All_trainseg, trainseg)\n",
        "    All_modelname = np.append(All_modelname, 'run_' + str(count) + '_UNIT_' + str(a) + '_LB_' + str(b) + '/' )\n",
        "    All_mod_name = np.append(All_mod_name, 'run_' + str(count) + '_UNIT_' + str(a) + '_LB_' + str(b) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVW0Xhdi8nP3"
      },
      "source": [
        "**Step 5**: Access and load trained model architectures in loop, extract the external and self-driven Hs and Cs and develop signatures. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKTh2iXu1z-5",
        "outputId": "a4c5727c-9435-4fc4-b898-9a9812681f1d"
      },
      "outputs": [],
      "source": [
        "for j in range(len(All_mod_name)):\n",
        "    \n",
        "    newfoldpath = path + All_mod_name[j]\n",
        "\n",
        "    # extract path to store each model and generated data\n",
        "    savepath = path + All_modelname[j]\n",
        "\n",
        "    mod_name = All_mod_name[j]\n",
        "    newfigpath = os.path.join(savepath, 'Trial_Prediction_Figures/')\n",
        "    #newfigpath = savepath + 'Model_Prediction_Figures'  # specify folder path to store predicted kinematics figures\n",
        "    try: # test if model folder already exists \n",
        "      if not os.path.exists(os.path.dirname(newfigpath)): # if fig folder does not exist create\n",
        "          os.makedirs(os.path.dirname(newfigpath)) # create a new fig folder\n",
        "    except OSError as err:\n",
        "          print(err)\n",
        "    \n",
        "    modnum = j+1 #model number counter for print statement\n",
        "    print('Working on: ' + mod_name + ' model ' + str(modnum) + ' / ' + str(len(All_mod_name)))\n",
        "    \n",
        "    # Specify variables for model run instance\n",
        "\n",
        "    # Number of Units\n",
        "    numunits = All_nodes[j]\n",
        "\n",
        "    # Lookback parameter\n",
        "    # A number is chosen called the look back parameter where this many samples are used to predict the outputs, calculate error, attain a gradient which is\n",
        "    # back propagatted to update the weights, then finally the weights are reset to zero. Our lookback parameter is typically 1 less than a divisor of the trial length. \n",
        "    # Thus, lookback + 1 should be divisible by the trialsamp. This set up is specifically for training input and output sequences that are one step time shifted (lag = 1)\n",
        "    # versions of eachother. E.g. trial length = 500 lookback = 99, input = samples 0:99, output = samples 1:100. Since the lookback + 1 (99 + 1) = 100, trial length/(lookback + 1) = 5;\n",
        "    # thus there are 5 mini-batches of input/output sequences per trial where we can evaluate error (resetting paramters each time).\n",
        "\n",
        "    lookback = All_seq[j]\n",
        "\n",
        "    # Training and Validation Set-up\n",
        "    # Based on the length of the trials and the lookback parameter you can set how many mini-batches would be used for training vs validation. \n",
        "    # For example, if the trial length is 1500 and num = lookback+1 = 250, since num can be divided into the trial length 6 times, there would be 6 minibatches.   \n",
        "    # One can specify that 4 of the mini-batches be used for training: trainseg = 4 corresponding to the 1st 4 mini-batches of the trial and 2 of the mini-batches used\n",
        "    # for validation: valseg = 2, corresponding to last 2 mini-batches of trial.\n",
        "    \n",
        "    # Select the 1st X segments to be training\n",
        "    trainseg = All_trainseg[j]\n",
        "    \n",
        "    # Select the last Y segments to be validation\n",
        "    valseg = All_valseg[j]\n",
        "\n",
        "    # load trained model from script 1 to save time re-training model\n",
        "    model = k.models.load_model(savepath + mod_name + '_bestwhole.h5') \n",
        "\n",
        "    # Need to be on GPU to run the following as well:\n",
        "\n",
        "    # Set the trained model weights to a new model to generate external and self-driven predictions\n",
        "    model2=k.models.Sequential()\n",
        "    model2.add(tf.compat.v1.keras.layers.CuDNNLSTM(units = numunits, stateful=True, return_sequences=True, batch_input_shape =(1,None,feats)))\n",
        "    model2.add(tf.compat.v1.keras.layers.Dense(units=6))\n",
        "    model2.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    # Set model 2 weights from model 1\n",
        "    for l1, l2 in zip(model.layers, model2.layers):\n",
        "      assert l1.get_config().keys()==l2.get_config().keys()\n",
        "    for key, v in l1.get_config().items():\n",
        "        if key=='name':\n",
        "            continue\n",
        "        if not v==l2.get_config()[key]:\n",
        "            print(l1.name, key, 'not matching.', 'Old,new : ', v, l2.get_config()[key], '\\n')\n",
        "\n",
        "    model2.set_weights(model.get_weights())\n",
        "    assert np.all([np.array_equal(w1, w2) for (w1,w2) in zip(model.get_weights(), model2.get_weights())]) \n",
        "\n",
        "    ####  Generate external and self driving signatures for all data\n",
        "    print('Running self-driven gait signatures')\n",
        "\n",
        "    in_data = all_inputdata_s\n",
        "    ext_drive= trialsamp # externally drive for twice the lookback\n",
        "    total_drive = trialsamp*2  # in self-driving mode the lower node models won't self-drive more than the look-back- they zero out pretty quickly \n",
        "\n",
        "    HC_self_concat = np.empty(shape=[0, numunits*2]) # initialize HC storage for self-driving\n",
        "    HC_ext_concat = np.empty(shape=[0, numunits*2])  # initialize HC storage for external-driving\n",
        "\n",
        "    Predicted_KIN = np.empty(shape = [len(in_data),total_drive, feats])\n",
        "    \n",
        "    test_ind = np.arange(0, len(all_inputdata_s), 1)  # generate kinematic predictions for all (or specified) indices of the input data\n",
        "    for g in test_ind:\n",
        "        trialnum = g+1\n",
        "        print('external and self-driven HCs parsed: ' + str(trialnum) + ' of ' + str(len(test_ind)))\n",
        "        model2.layers[0].reset_states([np.zeros((1,numunits)), np.zeros((1,numunits))])\n",
        "        preds = np.zeros((1, total_drive, 6))\n",
        "        hcs = np.zeros((2, total_drive, numunits))\n",
        "        for i in tqdm(range(total_drive)):\n",
        "            if i<ext_drive:\n",
        "                preds[:,i,:] = model2.predict(in_data[g,i,:][None,None,:])# index the test trial\n",
        "            else:\n",
        "                preds[:,i,:] = model2.predict(preds[0,i-1,:][None,None,:])\n",
        "\n",
        "            hcs[:,i,:] = np.array(model2.layers[0].states)[:,0]\n",
        "\n",
        "            Hvalues = hcs[0]\n",
        "            Cvalues = np.tanh(hcs[1])\n",
        "\n",
        "            Hvalues_self = Hvalues[ext_drive:total_drive]# only extract the selfdriving Hs\n",
        "            Hvalues_ext = Hvalues[0:ext_drive] # extract external Hs\n",
        "\n",
        "            Cvalues_self = Cvalues[ext_drive:total_drive]\n",
        "            Cvalues_ext = Cvalues[0:ext_drive]\n",
        "\n",
        "            HC_self = np.concatenate((Hvalues_self, Cvalues_self), axis=1) # concatenate Hs and Cs of single trial\n",
        "            HC_ext =  np.concatenate((Hvalues_ext, Cvalues_ext), axis=1)\n",
        "\n",
        "        HC_self_concat = np.append(HC_self_concat, HC_self, axis = 0) # concatenate all trials to eachother\n",
        "        HC_ext_concat = np.append(HC_ext_concat, HC_ext, axis = 0)\n",
        "        \n",
        "        prediction = preds[0,:,:] # external and self-driven prediction (external drive samples run from 0:ext_drive, self-driven samples run from ext_drive:ext_drive*2 )\n",
        "        in_data_trial = in_data[g] # trial reference kinematics\n",
        "\n",
        "        Predicted_KIN[g] = prediction # store predicted kinematics from each trial in loop\n",
        "        #np.save(savepath + mod_name + 'trial_' + str(g) + '_predictedkinematics.npy',  prediction) # save each trial of predicted kinematics\n",
        "        #np.save(savepath + mod_name + 'trial_' + str(g) + '_referencekinematics.npy', in_data_trial) #save each trial of original/reference kinematics\n",
        "\n",
        "        # Generate figure with predictive kinematics\n",
        "        fig = plt.figure(figsize=(20,15))\n",
        "        plt.subplot(711)\n",
        "        plt.title(\"Trial: \" + str(g), fontsize=14)\n",
        "        plt.plot(in_data_trial[:ext_drive+(trialsamp-ext_drive),0], \"b-\", markersize=8, label=\"Left Hip_input\") #original data\n",
        "        plt.plot(prediction[:,0], \"g-\", markersize=20, label=\"Left Hip_predict\") #predicted\n",
        "        plt.axvline(x=ext_drive, color='r', alpha=0.4)\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.xlabel(\"Sample Number\")\n",
        "        plt.ylabel(\"Angle\")\n",
        "\n",
        "        plt.subplot(712)\n",
        "        #plt.title(\"Evaluating Predictions\", fontsize=14)\n",
        "        plt.plot(in_data_trial[:ext_drive+(trialsamp-ext_drive),1], \"b-\", markersize=8, label=\"Left Knee_input\") #original data\n",
        "        plt.plot(prediction[:,1], \"g-\", markersize=20, label=\"Left Knee_predict\") #predicted\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.xlabel(\"Sample Number\")\n",
        "        plt.ylabel(\"Angle\")\n",
        "        plt.axvline(x=ext_drive, color='r', alpha=0.4)\n",
        "\n",
        "        plt.subplot(713)\n",
        "        #plt.title(\"Evaluating Predictions\", fontsize=14)\n",
        "        plt.plot(in_data_trial[:ext_drive+(trialsamp-ext_drive),2], \"b-\", markersize=8, label=\"Left Ank_input\") #original data\n",
        "        plt.plot(prediction[:,2], \"g-\", markersize=20, label=\"Left Ank_predict\") #predicted\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.xlabel(\"Sample Number\")\n",
        "        plt.ylabel(\"Angle\")\n",
        "        plt.axvline(x=ext_drive, color='r', alpha=0.4)\n",
        "\n",
        "        plt.subplot(714)\n",
        "        #plt.title(\"Evaluating Predictions\", fontsize=14)\n",
        "        plt.plot(in_data_trial[:ext_drive+(trialsamp-ext_drive),3], \"b-\", markersize=8, label=\"Right Hip_input\") #original data\n",
        "        plt.plot(prediction[:,3], \"g-\", markersize=20, label=\"Right Hip_predict\") #predicted\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.xlabel(\"Sample Number\")\n",
        "        plt.ylabel(\"Angle\")\n",
        "        plt.axvline(x=ext_drive, color='r', alpha=0.4)\n",
        "\n",
        "        plt.subplot(715)\n",
        "        #plt.title(\"Evaluating Predictions\", fontsize=14)\n",
        "        plt.plot(in_data_trial[:ext_drive+(trialsamp-ext_drive),4], \"b-\", markersize=8, label=\"Right Knee_input\") #original data\n",
        "        plt.plot(prediction[:,4], \"g-\", markersize=20, label=\"Right Knee_predict\") #predicted\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.xlabel(\"Sample Number\")\n",
        "        plt.ylabel(\"Angle\")\n",
        "        plt.axvline(x=ext_drive, color='r', alpha=0.4)\n",
        "\n",
        "        plt.subplot(716)\n",
        "        #plt.title(\"Evaluating Predictions\", fontsize=14)\n",
        "        plt.plot(in_data_trial[:ext_drive+(trialsamp-ext_drive),5], \"b-\", markersize=8, label=\"Right Ank_input\") #original data\n",
        "        plt.plot(prediction[:,5], \"g-\", markersize=20, label=\"Right Ank_predict\") #predicted\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.xlabel(\"Sample Number\")\n",
        "        plt.ylabel(\"Angle\")\n",
        "        plt.axvline(x=ext_drive, color='r', alpha=0.4)\n",
        "\n",
        "        plt.savefig(newfigpath + 'trial_' + str(trialnum) +  '_PredictiveKinematics.png', dpi = 300)\n",
        "        plt.close(fig)# close figure in loop\n",
        "\n",
        "    np.save(savepath + mod_name + '_PredictedKinematics_ext_self.npy', Predicted_KIN) # save predicted kinematics (external and self-dricen) from all trials\n",
        "\n",
        "    SelfDriveHCs = HC_self_concat # extracted self-driven HCs \n",
        "    ExternalDriveHCs = HC_ext_concat # extracted externally driven HCs\n",
        "\n",
        "    #CombinedExternal_Self_HCs = np.concatenate((ExternalDriveHCs,SelfDriveHCs), axis = 0) # all Hs and Cs\n",
        "\n",
        "    # save these held out HCs \n",
        "    np.save(savepath + mod_name + '_selfdriveHCs.npy', SelfDriveHCs)\n",
        "    scipy.io.savemat(savepath + mod_name + '_selfdriveHCs.mat', {'self_drive_sigs': SelfDriveHCs})\n",
        "\n",
        "    np.save(savepath + mod_name + '_extdriveHCs.npy', ExternalDriveHCs)\n",
        "    scipy.io.savemat(savepath + mod_name + '_extdriveHCs.mat', {'ext_drive_sigs': ExternalDriveHCs})\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPcjsyhCZnAAmoAJMwr5Ada",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}