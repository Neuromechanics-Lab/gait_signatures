{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0fGxPlhzo2jvfUIJasajH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bermanlabemory/gait_signatures/blob/main/Gait_Signatures_Script_3_Generate_Gait_Signatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This script accesses the extracted Hs and Cs from 'Gait Signatures Script 2: Extract Internal Parameters.ipynb' and generates phase averaged signatures. \n",
        "\n",
        "**notes:** \n",
        "1. Externally and Self-driven signatures are generated\n",
        "\n",
        "\n",
        "**Created by**: Taniel Winner\n",
        "\n",
        "**Date**: 07/*18*/22"
      ],
      "metadata": {
        "id": "onLC9Msr6_RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0**: Mount (connect to) your google drive folder where you want to save the simulation results and model parameters."
      ],
      "metadata": {
        "id": "kLBAyLXe7udu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "Iuzkec-P1z7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78baac84-b687-430d-a3d0-498ec446c2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check python version \n",
        "from platform import python_version\n",
        "\n",
        "print(python_version())"
      ],
      "metadata": {
        "id": "TfiYeSIR702R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd33f90-dfae-4304-b249-b1b472e821b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check tensorflow version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "h-gE34Gq705Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d42884-e39c-4e0f-9c69-5e3d55e203b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "jmVki5IZ708h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de68dc3-be67-4c90-d77d-9347c11a32f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**: Import necessary packages and functions to develop model"
      ],
      "metadata": {
        "id": "WDY0A5Qa77-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.model_selection as model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import keras as k\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import copy\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "from scipy import interpolate\n",
        "from scipy.special import iv\n",
        "from numpy import sin,cos,pi,array,linspace,cumsum,asarray,dot,ones\n",
        "from pylab import plot, legend, axis, show, randint, randn, std,lstsq\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "import csv\n",
        "import os\n",
        "from tqdm import tqdm "
      ],
      "metadata": {
        "id": "Sjnyu-Hi70_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/GaitSignature_Manuscript/\n",
        "\n",
        "# Ensure fourierseries.py is in the pathway\n",
        "!ls -l fourierseries.py"
      ],
      "metadata": {
        "id": "ZMRQ1Zrk_5m9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8e7b89-3084-4526-b63b-11b8426fc5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/GaitSignature_Manuscript\n",
            "-rw------- 1 root root 7259 Apr  4  2022 fourierseries.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fourierseries\n",
        "import util\n",
        "import phaser\n",
        "import dataloader\n",
        "# Preprocess data for a single subject - to be send to modeling frameworks\n",
        "def find_phase(k):\n",
        "    \"\"\"\n",
        "    Detrend and compute the phase estimate using Phaser\n",
        "    INPUT:\n",
        "      k -- dataframe\n",
        "    OUTPUT:\n",
        "      k -- dataframe\n",
        "    \"\"\"\n",
        "    #l = ['hip_flexion_l','hip_flexion_r'] # Phase variables = hip flexion angles\n",
        "    y = np.array(k)\n",
        "    print(y.shape)\n",
        "    y = util.detrend(y.T).T\n",
        "    print(y.shape)\n",
        "    phsr = phaser.Phaser(y=y)\n",
        "    k[:] = phsr.phaserEval(y)[0,:]\n",
        "    return k"
      ],
      "metadata": {
        "id": "956M3pr8_56k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vonMies(t,t_0, b):\n",
        "    out = np.exp(b*np.cos(t-t_0))/(2*pi*iv(0, b))\n",
        "    return out"
      ],
      "metadata": {
        "id": "UEWFCHvbAYc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: Load module in Google Drive"
      ],
      "metadata": {
        "id": "g4J5BZIg8EYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to save the models and read data from\n",
        "\n",
        "### !!! please create and name the main folder that you would like to save all results -- ensure that the functions and data files are in this folder\n",
        "folder = 'TestScripts_121422/'\n",
        "\n",
        "\n",
        "path = '/content/drive/My Drive/'+ folder\n",
        "\n",
        "# Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,path)"
      ],
      "metadata": {
        "id": "8PRNNDCI71CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Load in data and specify variables/parameters"
      ],
      "metadata": {
        "id": "DAhc9QkW8ONR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-changing variables \n",
        "\n",
        "# number of trials in dataset \n",
        "trialnum = 72 # 72 total trials\n",
        "\n",
        "# number of samples in each trial\n",
        "trialsamp = 1500\n",
        "\n",
        "# number of features collected per trial\n",
        "feats = 6\n",
        "\n",
        "#Batch size - same as the number of traintrials\n",
        "batch_size = trialnum\n",
        "\n",
        "# Number of Layers\n",
        "numlayers = 1\n",
        "\n",
        "# Choose the number of iterations to train the model- if this script has been run previously enter a value greater than was \n",
        "# inputted before and rerun the script. \n",
        "finalepoch = 10000\n",
        "\n",
        "# load the input data/kinematics\n",
        "datafilepath = '/content/drive/My Drive/GaitSignature_Manuscript/PareticvsNonP_RNNData.csv' #input data\n",
        "all_csvnp = np.loadtxt(datafilepath,delimiter=',').T\n",
        "\n",
        "# reshape all the input data into a tensor\n",
        "all_inputdata_s = all_csvnp.reshape(trialnum,trialsamp,feats) \n",
        "\n",
        "print('original input data shape is: '+ str(all_csvnp.shape ))\n",
        "print('input data reshaped is: '+ str(all_inputdata_s.shape))"
      ],
      "metadata": {
        "id": "fXEEUgWB8Mk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959a3258-6d32-4338-e415-6d252aa99070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original input data shape is: (108000, 6)\n",
            "input data reshaped is: (72, 1500, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**: List of model architectures and corresponding variables from script 1 and 2"
      ],
      "metadata": {
        "id": "q_A7pitQ8XS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a list of models and corresponding parameters to test \n",
        "test_model_nodes = [512] \n",
        "seqs = [249] #lookback parameter\n",
        "\n",
        "# run multiple model architechtures many times to test stability of cost function outputs\n",
        "runs = 1 # stability analysis - repeat each model architecture this many times\n",
        "test_model_seq = np.repeat(seqs, runs)\n",
        "\n",
        "All_nodes = np.empty([0,1], dtype='int')\n",
        "All_seq = np.empty([0,1],dtype='int')\n",
        "All_valseg = np.empty([0,1],dtype='int')\n",
        "All_trainseg = np.empty([0,1],dtype='int')\n",
        "All_modelname = []\n",
        "All_mod_name = []\n",
        "\n",
        "count = 0; #initialize model run -- this serves as the model run ID number\n",
        "for a in test_model_nodes:\n",
        "  for b in test_model_seq:\n",
        "    if count < runs:\n",
        "      count = count + 1 \n",
        "    else: \n",
        "      count = 1 # reset counter when all runs of certain model attained\n",
        "    #if statement for valseg, trainseg based on sequence length\n",
        "    if int(b) == 249:\n",
        "      trainseg = 4\n",
        "      valseg = 2\n",
        "    elif int(b) == 499: \n",
        "      trainseg = 2\n",
        "      valseg = 1\n",
        "    elif int(b) == 749:\n",
        "      trainseg = 1\n",
        "      valseg = 1\n",
        "\n",
        "    All_nodes = np.append(All_nodes, a) \n",
        "    All_seq = np.append(All_seq, int(b))\n",
        "    All_valseg = np.append(All_valseg, valseg)\n",
        "    All_trainseg = np.append(All_trainseg, trainseg)\n",
        "    All_modelname = np.append(All_modelname, 'run_' + str(count) + '_UNIT_' + str(a) + '_LB_' + str(b) + '/' )\n",
        "    All_mod_name = np.append(All_mod_name, 'run_' + str(count) + '_UNIT_' + str(a) + '_LB_' + str(b) )"
      ],
      "metadata": {
        "id": "vAN8l21V8ZeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**: Access and load trained model architectures in loop, extract the external and self-driven Hs and Cs and develop signatures. "
      ],
      "metadata": {
        "id": "rVW0Xhdi8nP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(All_mod_name)):\n",
        "    # make folder to store model\n",
        "    newfoldpath = path + All_mod_name[j]\n",
        "\n",
        "    # extract path to store each model and generated data\n",
        "    savepath = path + All_modelname[j]\n",
        "    mod_name = All_mod_name[j]\n",
        "\n",
        "    print('Working on: ' + mod_name + ' model ' + str(j) + ' / ' + str(len(All_mod_name)))\n",
        "    \n",
        "    # Specify variables for model run instance\n",
        "\n",
        "    # Number of Units\n",
        "    numunits = All_nodes[j]\n",
        "    \n",
        "    test_ind = np.arange(0, len(all_inputdata_s), 1)  # test indices: run through all input data\n",
        "    ext_drive= trialsamp # length of externally driven prediction\n",
        "\n",
        "    # PCA of HCs\n",
        "    ExternalDriveHCs_ = np.load(savepath + mod_name + '_extdriveHCs.npy')\n",
        "    SelfDriveHCs = np.load(savepath + mod_name + '_selfdriveHCs.npy')\n",
        "\n",
        "    X = ExternalDriveHCs # PCA of only externally driven predictions [used to make Gait Signatures]\n",
        "\n",
        "    pca = PCA(n_components=numunits*2) #initialize\n",
        "\n",
        "    X_reduction = pca.fit_transform(X) # original data is projected onto the derived principal components from pca\n",
        "\n",
        "    np.save(savepath + mod_name + '_pca_var.npy', pca.explained_variance_ratio_) #save variance explained by PCs\n",
        "    #scipy.io.savemat(savepath + mod_name + '_pca_var_all.mat', {'Variance_exp': pca.explained_variance_ratio_}) #save to Matlab\n",
        "\n",
        "    tot_len = len(test_ind) # total number of trials in dataset\n",
        "    HC_CellArray = np.empty(shape=[tot_len, ext_drive, X_reduction.shape[1]])\n",
        "    start = 0\n",
        "    last = ext_drive # length of trials\n",
        "    for p in range(tot_len):\n",
        "        HC_CellArray[p]= X_reduction[start:last]\n",
        "        start = start+ext_drive\n",
        "        last = last+ext_drive\n",
        "\n",
        "    #scipy.io.savemat(savepath + mod_name + '_HCvalues.mat', {'HC_CellArray': HC_CellArray}) save HC time traces to matlab\n",
        "\n",
        "    # Generate phase averaged signatures\n",
        "    print('generating phase averaged signatures')\n",
        "    # phase length = 100\n",
        "    lim = tot_len\n",
        "    L = 6 # lets look at 1st 6 to speed things up #numunits*2  # number of PCs or units to be phase averaged\n",
        "    PhaseAveragedPCs = np.empty(shape=[lim, L, 100])\n",
        "    PhaseAveragedPCs_shift = np.empty(shape=[lim, L, 100])\n",
        "    Phase_Variables = np.empty(shape=[lim, 1500])\n",
        "    cyclephase = []\n",
        "    PC_Shifts = np.empty(shape=[lim]) # store phase shift values\n",
        "    # Average orbits initialization\n",
        "    numSegments = 100 # we want each Phase averaged orbit to be 100 sample points long\n",
        "    phaseVals = np.linspace(0, 2*pi, numSegments, endpoint=True) # phases that we want our phase averaged orbits to correspond to\n",
        "    kappa = 20 \n",
        "\n",
        "    for a in range(lim): #for length of all the trials \n",
        "        print(['processing trial: ' + str(a)]) \n",
        "        dats=[] # reset variables after each trial - kinematics\n",
        "        dats2 = [] # reset the HC params\n",
        "        allsigs = np.empty(shape=[100,])\n",
        "        all_phase_var = np.empty(shape=[100,]) \n",
        "        PhaseAvgPCs = np.empty(shape=[100])\n",
        "        all_cyclephase = []\n",
        "        raw = HC_CellArray[a][:,0:3].T # Only use 1st 3 PCs (of HCs),\n",
        "        raw2 = HC_CellArray[a][:,0:6].T  # extract all the HC data\n",
        "        for b in range(6): #Shai's code does this duplication- works better than without duplicating data\n",
        "            dats.append(raw-raw.mean(axis=1)[:,np.newaxis]) #center the HCs data for phase estimation -- this centers each one separately\n",
        "            dats2.append(raw2-raw2.mean(axis=1)[:,np.newaxis])\n",
        "        #rHC = dats2[0] #centered data\n",
        "        rHC = raw2 #uncentered data\n",
        "        phr = phaser.Phaser(dats) # use centered data from 1st 3 PCs\n",
        "        phi = [ phr.phaserEval( d ) for d in dats ] # extract phase\n",
        "        phi2  =(phi[0].T % (2*pi)); # Take modulo s.t. phase ranges from [0,2*pi]\n",
        "    \n",
        "        # find average orbits\n",
        "        avgOrbits = np.zeros((numSegments,L)) #initialize avg orbits\n",
        "        phases = np.reshape(phi2,[1500,]) \n",
        "\n",
        "        for c in range(numSegments): #number of points in final average orbit/phase averaging\n",
        "            vonMiesCurrent = vonMies(phases,phaseVals[c],kappa) # for each value of phase points calculate current\n",
        "            sumVal = np.sum(vonMiesCurrent)\n",
        "\n",
        "            for d in range(L):  # for the number of features or units\n",
        "                data = np.reshape(rHC[d,:],[trialsamp,1])\n",
        "                avgOrbits[c,d] = np.sum(data.T*vonMiesCurrent)/sumVal # phase point (row), feature of PC (column) -- this is generating a value for the 1st phase point of each feature\n",
        "\n",
        "        PhaseAveragedPCs[a] = avgOrbits.T # transform to match overall saving structure as before\n",
        "        phi2 = phi2.reshape(1500,)\n",
        "        Phase_Variables[a] = phi2 # store phase variables for all cycles/features - may need to phase shift these \n",
        "        \n",
        "        # Phase shift the data according to PC1 max align with zero phase\n",
        "        PC1_maxloc = np.argmax(PhaseAveragedPCs[a][0]) # identify the max PC1 value (1st max if repeated)\n",
        "        Data2Shift = PhaseAveragedPCs[a] \n",
        "        NewP = np.roll(Data2Shift, -PC1_maxloc,axis=1) #shift data so that PC1 max is at the orgin - this is defined as zero phase\n",
        "        PC_Shifts[a] = PC1_maxloc\n",
        "        PhaseAveragedPCs_shift[a] = NewP #store shifted data\n",
        "\n",
        "\n",
        "    np.save(savepath + mod_name + '_PhaseAvgPcs.npy', PhaseAveragedPCs) #unshifted\n",
        "    scipy.io.savemat(savepath + mod_name + '_PhaseAvgPcs.mat', {'PhaseAvgPCs': PhaseAveragedPCs})\n",
        "\n",
        "    np.save(savepath + mod_name + '_PhaseAvgPcs_shift.npy', PhaseAveragedPCs_shift) #shifted to set phase = 0 as the max of PC1\n",
        "    scipy.io.savemat(savepath + mod_name + '_PhaseAvgPcs_shift.mat', {'PhaseAvgPCs_shift': PhaseAveragedPCs_shift})\n",
        "\n",
        "    np.save(savepath + mod_name + '_PhaseVariables.npy', Phase_Variables)\n",
        "    scipy.io.savemat(savepath + mod_name + '_PhaseVariables.mat', {'PhaseVariables': Phase_Variables})\n",
        "\n",
        "\n",
        "    # Concatenate gait signature per trial\n",
        "    # Make a single phase averaged signature per row\n",
        "    PA_shape = PhaseAveragedPCs_shift.shape\n",
        "    gait_sig_size = PA_shape[1]*PA_shape[2]\n",
        "    Gaitsignatures = np.empty(shape = [len(PhaseAveragedPCs_shift),gait_sig_size]) # Store gait signatures\n",
        "    Gaitsignatures_trunc6 = np.empty(shape = [len(PhaseAveragedPCs_shift),600]) # Store truncated version of gait signatures to the first 6 PCs\n",
        "    for h in range(len(PhaseAveragedPCs_shift)):\n",
        "        reshape_sig = PhaseAveragedPCs_shift[h].reshape(1,gait_sig_size)\n",
        "        Gaitsignatures[h] = reshape_sig[0]\n",
        "        Gaitsignatures_trunc6[h] = reshape_sig[0][0:600] #truncate to 1st 6 PCs\n",
        "    np.save(savepath + mod_name + '_Gaitsignatures.npy', Gaitsignatures)\n",
        "    scipy.io.savemat(savepath + mod_name + '_Gaitsignatures.mat', {'GaitSigs': Gaitsignatures})\n",
        "\n",
        "    np.save(savepath + mod_name + '_Gaitsignatures_trunc6.npy', Gaitsignatures_trunc6)\n",
        "    scipy.io.savemat(savepath + mod_name + '_Gaitsignatures_trunc6.mat', {'GaitSigs': Gaitsignatures_trunc6})\n",
        "\n",
        "\n",
        "    # Import list of trials' corresponding speed labels\n",
        "    datafilepath3 = '/content/drive/My Drive/GaitSignature_Manuscript/SpeedLabels.mat'\n",
        "    speedlabels = scipy.io.loadmat(datafilepath3)\n",
        "\n",
        "    speedlist = []\n",
        "    for m in test_ind:\n",
        "        speedlist.append(speedlabels['SpeedTrainTrials'][m][0][0][0])\n",
        "\n",
        "    myfile = '/content/drive/My Drive/GaitSignature_Manuscript/Subjectlabels.csv'\n",
        "    csvfile = open(myfile, 'r')\n",
        "    reader = csv.reader(csvfile, delimiter='\\t')\n",
        "    allsubs = list(reader)\n",
        "    select_subs = map(allsubs.__getitem__, test_ind)\n",
        "    subs= list(select_subs)\n",
        "\n",
        "    # Mutlidimensional scaling of gait signatures\n",
        "    embedding = MDS(n_components=2)\n",
        "    X_transformed = embedding.fit_transform(Gaitsignatures_trunc6) # Perform MDS on 6D gait signatures to visualize in 2D\n",
        "    np.save(savepath + mod_name + '_MDS_X_transformed.npy', X_transformed)\n",
        "\n",
        "    # MDS plots of ext and self sigs\n",
        "    fig = plt.figure()\n",
        "    plt.scatter(X_transformed[:,0],X_transformed[:,1],c = speedlist) # color 2D gait signatures according to speed\n",
        "    # Loop for annotation of all points\n",
        "    count = 0\n",
        "    for i in range(len(subs)):\n",
        "        plt.annotate(str(count), (X_transformed[i,0], X_transformed[i,1] + 0.3)) # annotate subject ID next to the 2D gait signatures\n",
        "        count = count + 1    \n",
        "    plt.savefig(savepath + mod_name +  '_MDS.png', dpi = 300)\n",
        "    plt.close(fig)# close figure in loop"
      ],
      "metadata": {
        "id": "DKTh2iXu1z-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "834f926a-52d9-4098-fbb3-304575749962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on: run_1_UNIT_512_LB_249 model 0 / 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d41b90486926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# PCA of HCs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mExternalDriveHCs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmod_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_extdriveHCs.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mSelfDriveHCs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmod_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_selfdriveHCs.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/TestScripts_121422/run_1_UNIT_512_LB_249/run_1_UNIT_512_LB_249_extdriveHCs.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3D Loops of Able-bodied and stroke Gait signatures"
      ],
      "metadata": {
        "id": "8MS5lDXlayf5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0slS9m5sa4BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2D MDS map of all gait signatures colored by walking speed and annotated by individual label"
      ],
      "metadata": {
        "id": "C_hC42zga4UD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7VD1gfOZa-9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}