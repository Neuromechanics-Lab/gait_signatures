{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bermanlabemory/gait_signatures/blob/main/Gait_Signatures_Script_5_Compute_Gait_Signature_Alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Gait Signature Alignment for short- and long0time predictions. \n",
        "____________\n",
        "____________\n",
        "**NOTE**: This file was ONLY used to select model hyperparameters and generate supplementary figures.\n",
        "\n",
        "First, Scripts 1-4 must be run to generate the models that are analyzed here.\n",
        "____________\n",
        "____________\n",
        "**Steps:** \n",
        "Short-time alignment:\n",
        "1. Load short-time externally- and self-driven internal parameters. Note that the externally- and self-driven domains are the same - no phase-averaging needed.\n",
        "2. Project into the same low-D basis using Principal Components Analysis (PCA)\n",
        "2. Compute R-squared of the signatures\n",
        "4. Save the R2 values\n",
        "____________\n",
        "Long-time alignment\n",
        "1. Load long-time externally- and self-driven internal parameters. Here, the domains on the externally- and self-driven signatures differ, such that we need to phase average the data to compare trajectories.\n",
        "2. Project into the same low-D basis using Principal Components Analysis (PCA). This gives gait signature trajectories in time.\n",
        "2. Estimate gait phase and phase average externally- and self-driven signatures separately\n",
        "2. Compute R-squared of the phase-averaged gait signatures\n",
        "4. Save the R2 values\n",
        "___________\n",
        "Note that, for short and long-time alignment, Models that fail to predict rhythmic oscillations are given NaN values. An arbitrary negative R2 value could also be computed.\n",
        "____________\n",
        "____________\n",
        "\n",
        "**Created by**: Michael C. Rosenberg\n",
        "\n",
        "**Date**: 09/22/22"
      ],
      "metadata": {
        "id": "onLC9Msr6_RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0**: Mount (connect to) your google drive folder where you want to save the simulation results and model parameters."
      ],
      "metadata": {
        "id": "kLBAyLXe7udu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "Iuzkec-P1z7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5370b02-835b-4b04-f734-d2a175fd4cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check python version \n",
        "from platform import python_version\n",
        "print(python_version())\n",
        "\n",
        "# check tensorflow version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# Check memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "TfiYeSIR702R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9dff702-6891-4328-cc44-a509eafce6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**: Import necessary packages and functions to develop model"
      ],
      "metadata": {
        "id": "WDY0A5Qa77-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# from keras.layers.recurrent import LSTM # Deprecated :(\n",
        "from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.model_selection as model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import keras as k\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import copy\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "from scipy import interpolate\n",
        "from scipy.special import iv\n",
        "from numpy import sin,cos,pi,array,linspace,cumsum,asarray,dot,ones\n",
        "from pylab import plot, legend, axis, show, randint, randn, std,lstsq\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "import csv\n",
        "import os\n",
        "from tqdm import tqdm "
      ],
      "metadata": {
        "id": "Sjnyu-Hi70_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/NeuromechanicsLab/GaitSignatures/\n",
        "\n",
        "# Ensure fourierseries.py is in the pathway\n",
        "!ls -l fourierseries.py"
      ],
      "metadata": {
        "id": "ZMRQ1Zrk_5m9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61540fb9-c3b1-4152-9b3c-3a9a8d10b7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/NeuromechanicsLab/GaitSignatures\n",
            "-rw------- 1 root root 7259 May 29  2020 fourierseries.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fourierseries\n",
        "import util\n",
        "import phaser\n",
        "import dataloader\n",
        "# Preprocess data for a single subject - to be send to modeling frameworks\n",
        "def find_phase(k):\n",
        "    \"\"\"\n",
        "    Detrend and compute the phase estimate using Phaser\n",
        "    INPUT:\n",
        "      k -- dataframe\n",
        "    OUTPUT:\n",
        "      k -- dataframe\n",
        "    \"\"\"\n",
        "    #l = ['hip_flexion_l','hip_flexion_r'] # Phase variables = hip flexion angles\n",
        "    y = np.array(k)\n",
        "    print(y.shape)\n",
        "    y = util.detrend(y.T).T\n",
        "    print(y.shape)\n",
        "    phsr = phaser.Phaser(y=y)\n",
        "    k[:] = phsr.phaserEval(y)[0,:]\n",
        "    return k"
      ],
      "metadata": {
        "id": "956M3pr8_56k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vonMies(t,t_0, b):\n",
        "    out = np.exp(b*np.cos(t-t_0))/(2*pi*iv(0, b))\n",
        "    return out"
      ],
      "metadata": {
        "id": "UEWFCHvbAYc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: Load module in Google Drive"
      ],
      "metadata": {
        "id": "g4J5BZIg8EYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to save the models and read data from\n",
        "path = '/content/drive/My Drive/NeuromechanicsLab/GaitSignatures/'\n",
        "\n",
        "# Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,path)"
      ],
      "metadata": {
        "id": "8PRNNDCI71CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Load in data and specify variables/parameters"
      ],
      "metadata": {
        "id": "DAhc9QkW8ONR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-changing variables \n",
        "\n",
        "# number of trials in dataset \n",
        "trialnum = 72 # 72 total trials\n",
        "\n",
        "# number of samples in each trial\n",
        "trialsamp = 1500\n",
        "\n",
        "# number of features collected per trial\n",
        "feats = 6\n",
        "\n",
        "#Batch size - same as the number of traintrials\n",
        "batch_size = trialnum\n",
        "\n",
        "# Number of Layers\n",
        "numlayers = 1\n",
        "\n",
        "# Choose the number of iterations to train the model- if this script has been run previously enter a value greater than was \n",
        "# inputted before and rerun the script. \n",
        "finalepoch = 10000\n",
        "\n",
        "# load the input data/kinematics\n",
        "datafilepath = path + 'PareticvsNonP_RNNData.csv' #input data\n",
        "all_csvnp = np.loadtxt(datafilepath,delimiter=',').T\n",
        "\n",
        "# reshape all the input data into a tensor\n",
        "all_inputdata_s = all_csvnp.reshape(trialnum,trialsamp,feats) \n",
        "csvnp = all_inputdata_s\n",
        "print('original input data shape is: '+ str(all_csvnp.shape ))\n",
        "print('input data reshaped is: '+ str(all_inputdata_s.shape))"
      ],
      "metadata": {
        "id": "fXEEUgWB8Mk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d0fab5-ed5d-4ec9-9b02-47211f827a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original input data shape is: (108000, 6)\n",
            "input data reshaped is: (72, 1500, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**: Develop list of model architectures and corresponding variables to train. This step also generates list of folder names and pathways where the models will be saved and accessed later."
      ],
      "metadata": {
        "id": "q_A7pitQ8XS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a list of models and corresponding parameters to test \n",
        "test_model_nodes = [128, 256,512,1024] \n",
        "seqs = [249,499,749] #lookback parameter\n",
        "\n",
        "test_model_nodes = [ 512] \n",
        "seqs = [499] #lookback parameter\n",
        "\n",
        "# run multiple model architechtures many times to test stability of cost function outputs\n",
        "runs = 1 # Number of times to train recurrent neural network (RNN) models, starting from random initial conditions. We used this for hyperparameter selection\n",
        "test_model_seq = np.repeat(seqs, runs) # Specify each model's hyperparameters\n",
        "\n",
        "count = np.arange(runs)\n",
        "\n",
        "All_nodes = np.empty([0,1], dtype='int')\n",
        "All_seq = np.empty([0,1],dtype='int')\n",
        "All_valseg = np.empty([0,1],dtype='int')\n",
        "All_trainseg = np.empty([0,1],dtype='int')\n",
        "All_modelname = []\n",
        "All_mod_name = []\n",
        "count = np.empty([0,1],dtype='int'); #initialize model run -- this serves as the model run ID number\n",
        "ct = 0\n",
        "for a in test_model_nodes:\n",
        "  for b in test_model_seq:\n",
        "    count = np.append(count,  ct + 1 )\n",
        "    #if statement for valseg, trainseg based on sequence length\n",
        "    if int(b) == 249:\n",
        "      trainseg = 4\n",
        "      valseg = 2\n",
        "    elif int(b) == 499: \n",
        "      trainseg = 2\n",
        "      valseg = 1\n",
        "    elif int(b) == 749:\n",
        "      trainseg = 1\n",
        "      valseg = 1\n",
        "\n",
        "    # Store resulting model structures and training plans\n",
        "    All_nodes = np.append(All_nodes, a) \n",
        "    All_seq = np.append(All_seq, int(b))\n",
        "    All_valseg = np.append(All_valseg, valseg)\n",
        "    All_trainseg = np.append(All_trainseg, trainseg)\n",
        "    All_modelname = np.append(All_modelname, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) + '/' )\n",
        "    All_mod_name = np.append(All_mod_name, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) )\n",
        "\n",
        "    if ct+1 < runs:\n",
        "      ct += 1\n",
        "    else:\n",
        "      ct = 0"
      ],
      "metadata": {
        "id": "vAN8l21V8ZeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa85499-6c99-4a24-9756-f1e068b1f7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "['UNIT_256_LB_499_run_1' 'UNIT_256_LB_499_run_2' 'UNIT_256_LB_499_run_3'\n",
            " 'UNIT_256_LB_499_run_4' 'UNIT_256_LB_499_run_5' 'UNIT_256_LB_499_run_6'\n",
            " 'UNIT_256_LB_499_run_7' 'UNIT_256_LB_499_run_8' 'UNIT_256_LB_499_run_9'\n",
            " 'UNIT_256_LB_499_run_10' 'UNIT_256_LB_749_run_1' 'UNIT_256_LB_749_run_2'\n",
            " 'UNIT_256_LB_749_run_3' 'UNIT_256_LB_749_run_4' 'UNIT_256_LB_749_run_5'\n",
            " 'UNIT_256_LB_749_run_6' 'UNIT_256_LB_749_run_7' 'UNIT_256_LB_749_run_8'\n",
            " 'UNIT_256_LB_749_run_9' 'UNIT_256_LB_749_run_10' 'UNIT_512_LB_499_run_1'\n",
            " 'UNIT_512_LB_499_run_2' 'UNIT_512_LB_499_run_3' 'UNIT_512_LB_499_run_4'\n",
            " 'UNIT_512_LB_499_run_5' 'UNIT_512_LB_499_run_6' 'UNIT_512_LB_499_run_7'\n",
            " 'UNIT_512_LB_499_run_8' 'UNIT_512_LB_499_run_9' 'UNIT_512_LB_499_run_10']\n",
            "/content/drive/My Drive/NeuromechanicsLab/GaitSignatures/UNIT_256_LB_499_run_1/UNIT_256_LB_499_run_1_bestwhole.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute SHORT-time gait signature alignment\n",
        "1. Load short-time self- and externally-driven internal states (H & C values) from Script 4. These samples are paired.\n",
        "2. Run PCA to project all data into the same basis.\n",
        "2. Compute the mean squared error between self- and ext-driven signatures (note: need to use the same samples for all initializations -> need to extract non-zero indices that are common across all initializations for a fair comparison)\n",
        "2. Compare r-squared of prediction accuracy (note: MSE increases and r2 decreases with increasing number of samples (N). Both change by a factor of N2/N1). Report N2/N1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rVW0Xhdi8nP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-allocate arrays\n",
        "err = np.empty((len(All_mod_name), 1))\n",
        "sigAlign = np.zeros((trialnum, 1))\n",
        "R2 = np.zeros((trialnum, 1))  # THIS IS OUR ALIGNMENT OUTCOME\n",
        "\n",
        "\n",
        "for j in range(len(All_mod_name)): # For each model\n",
        "  plt.figure(figsize = (25,25))\n",
        "\n",
        "  # make folder to store model\n",
        "  newfoldpath = path + All_mod_name[j]\n",
        "\n",
        "  # extract path to store each model and generated data\n",
        "  savepath = path + All_modelname[j]\n",
        "  mod_name = All_mod_name[j]\n",
        "  print('Working on: ' + mod_name + ' model ' + str(j) + ' / ' + str(len(All_mod_name)))\n",
        "\n",
        "  # Number of Units\n",
        "  numunits = All_nodes[j]\n",
        "  test_ind = np.arange(0, len(all_inputdata_s), 1)  # run all input data through\n",
        "  ext_drive= trialsamp\n",
        "  tot_len = len(test_ind) # len of all trials\n",
        "\n",
        "  # Load Short-time gait signature H & C's and concatenate\n",
        "  ExtSelfHCs = scipy.io.loadmat(savepath + mod_name + '_selfExtDriveHC_shortTime.mat')\n",
        "  ExtHC = ExtSelfHCs['HC_ext_concat']\n",
        "  SelfHC = ExtSelfHCs['HC_self_concat']\n",
        "\n",
        "  # Project into a common low-D basis using PCA\n",
        "  pca = PCA(n_components=numunits*2) #initialize\n",
        "  pca.fit(ExtHC)\n",
        "  ExtSig = pca.transform(ExtHC) # X is projected on the first principal components previously extracted from a training set - data is centered here\n",
        "  SelfSig = pca.transform(SelfHC) \n",
        "\n",
        "  # Compute gait sig alignment and R2 for each trials\n",
        "  predLen = int(np.shape(SelfHC)[0]/trialnum)\n",
        "  for tr in range(trialnum):\n",
        "    inds = range( (tr*predLen),  ((tr+1)*predLen) )\n",
        "    # Get this metric as a sanity check and a way to discard bogus trials later\n",
        "    sigAlign[tr,0] = np.linalg.norm( np.reshape(ExtSig[inds,:] - SelfSig[inds,:], (-1,1))) # Norm of difference b/t Externally and self-driven signatures\n",
        "\n",
        "    # Compute R-squared (R2) - THIS IS OUR OUTCOME\n",
        "    SSR = np.sum( (np.reshape(ExtSig[inds,:],(-1,1)) - np.reshape(SelfSig[inds,:], (-1,1) ) )**2 )\n",
        "    SST = np.sum( (np.reshape(ExtSig[inds,:],(-1,1)) - np.mean(np.reshape(ExtSig[inds,:], (-1,1) )) )**2 )\n",
        "    R2[tr,0] = 1 - SSR/SST\n",
        "    if R2[tr,0] > 0.99:\n",
        "      R2[tr,0] = np.nan\n",
        "\n",
        "    # Plot\n",
        "    ax = plt.subplot(12,6,tr+1)\n",
        "    ax.plot(ExtSig[inds,0],'-')\n",
        "    ax.plot(SelfSig[inds,0],'-')\n",
        "    if tr > 65: \n",
        "      plt.xlabel('Time')\n",
        "    else:\n",
        "      ax.set_xticklabels([])\n",
        "      ax.set_xticks([])\n",
        "    \n",
        "    plt.title('R2 = ' + str(np.round(R2[tr,0],2)) + ' | ' + 'Alignment = ' + str(np.round(sigAlign[tr,0],0)))\n",
        "\n",
        "  scipy.io.savemat(savepath + mod_name + '_shortTimeAlignment.mat',{'EuclideanAlignment': sigAlign,'R2': R2})\n",
        "  plt.savefig(savepath + 'selfDrivenPredictions.png', dpi = 150)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "E2UNrdO2Ziw1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "db8a7401-6f27-4e1b-d97d-805de43e6367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on: UNIT_256_LB_499_run_1 model 0 / 30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7797a9cfa429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m# Project into a common low-D basis using PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumunits\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtHC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mExtSig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtHC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# X is projected on the first principal components previously extracted from a training set - data is centered here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mSelfSig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelfHC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# X is projected on the first principal components previously extracted from a training set - data is centered here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \"\"\"\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"arpack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"randomized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;31m# flip eigenvectors' sign to enforce deterministic output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0;32m--> 128\u001b[0;31m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x1800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long-time predictions\n",
        "\n",
        "1. Load ext & self-diven gait signatures\n",
        "2. Compute PCs for externally driven signatures. This is necessary because self-driven signatures that converge to zero can cause problems.\n",
        "\n",
        "3. Project externally- and self-driven sigs into that the same PC basis\n",
        "\n",
        "3. Phase average the ext-driven signatures\n",
        "5. Compute long-time gait sig alignment (alignment of the average signatures)"
      ],
      "metadata": {
        "id": "OXNEJAXx65hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(28,len(All_mod_name)):\n",
        "  R2 = np.zeros((trialnum, 1)) # Pre-allocate R-squared array)\n",
        "\n",
        "  # Create figure for sanity chech\n",
        "  plt.figure(figsize= (20,40))\n",
        "\n",
        "  # make folder to store model\n",
        "  newfoldpath = path + All_mod_name[j]\n",
        "\n",
        "  # extract path to store each model and generated data\n",
        "  savepath = path + All_modelname[j]\n",
        "  mod_name = All_mod_name[j]\n",
        "  print('Working on: ' + mod_name + ' model ' + str(j) + ' / ' + str(len(All_mod_name)))\n",
        "\n",
        "  # Number of Units\n",
        "  numunits = All_nodes[j]\n",
        "  test_ind = np.arange(0, len(all_inputdata_s), 1)  # run all input data through\n",
        "  ext_drive= trialsamp\n",
        "  tot_len = len(test_ind) # len of all trials\n",
        "\n",
        "  # Load H & C and concatenate\n",
        "  ExternalDriveHCs_testset = scipy.io.loadmat(savepath + mod_name + '_extdriveHCs_testset.mat')\n",
        "  ExternalDriveHCs_testset = ExternalDriveHCs_testset['ext_drive_sigs']\n",
        "  SelfDriveHCs_testset = scipy.io.loadmat(savepath + mod_name + '_selfdriveHCs.mat')\n",
        "  SelfDriveHCs_testset = SelfDriveHCs_testset['self_drive_sigs']\n",
        "\n",
        "  # Concatenate -[Ext,Self] for each trial\n",
        "  CombinedExternal_Self_HCs = np.zeros( (2*np.shape(ExternalDriveHCs_testset)[0], np.shape(ExternalDriveHCs_testset)[1]) )\n",
        "  for jj in range(72):\n",
        "    inds1 = range(jj*1500,(jj+1)*1500)\n",
        "    inds2 = range(jj*3000,(jj+1)*3000)\n",
        "    CombinedExternal_Self_HCs[inds2,:] = np.concatenate( (ExternalDriveHCs_testset[inds1,:], SelfDriveHCs_testset[inds1,:]), axis = 0)\n",
        "\n",
        "  # Project into a common low-D basis using PCA\n",
        "  pca = PCA(n_components=numunits*2) #initialize\n",
        "  pca.fit(ExternalDriveHCs_testset)\n",
        "  X_reduction = pca.transform(CombinedExternal_Self_HCs) # X is projected on the first principal components previously extracted from a training set - data is centered here\n",
        "  np.save(savepath + mod_name + '_pca_vaf_extSelfDrive.npy', pca.explained_variance_ratio_[0:6])\n",
        "\n",
        "  ###########################################################################\n",
        "  ###########################################################################\n",
        "  ###########################################################################\n",
        "\n",
        "  # Get  and allocate arrays\n",
        "  HC_CellArray = np.empty(shape=[tot_len, ext_drive*2, X_reduction.shape[1]])\n",
        "  start = 0\n",
        "  last = ext_drive*2 # length of trials\n",
        "  print('X_reduction shape = ' + str(np.shape(X_reduction)))\n",
        "  for p in range(tot_len):\n",
        "      HC_CellArray[p]= X_reduction[start:last]\n",
        "      start = start+ext_drive*2\n",
        "      last = last+ext_drive*2\n",
        "\n",
        "  # Generate phase averaged signatures\n",
        "  print('generating phase averaged signatures')\n",
        "  numSegments = 100 # Number of samples in one cycle\n",
        "  lim = tot_len\n",
        "  L = HC_CellArray.shape[2] # Number of PCs to phase average\n",
        "\n",
        "  # Allocate external and self-driving arrays\n",
        "  PhaseAveragedPCsExt = np.empty(shape=[lim, L, numSegments])\n",
        "  PhaseAveragedPCs_shiftExt = np.empty(shape=[lim, L, numSegments])\n",
        "  Phase_VariablesExt = np.empty(shape=[lim, ext_drive])\n",
        "  PC_ShiftsExt = np.empty(shape=[lim]) # store phase shift values\n",
        "\n",
        "  PhaseAveragedPCsSelf = np.empty(shape=[lim, L, numSegments])\n",
        "  PhaseAveragedPCs_shiftSelf = np.empty(shape=[lim, L, numSegments])\n",
        "  Phase_VariablesSelf = np.empty(shape=[lim, ext_drive])\n",
        "  PC_ShiftsSelf = np.empty(shape=[lim]) # store phase shift values\n",
        "\n",
        "  cyclephase = [] # Phase of the gait cycle - needed to align the cycles \n",
        "\n",
        "  # Average orbits initialization\n",
        "  phaseVals = np.linspace(0, 2*pi, numSegments, endpoint=True) # phases that we want our phase averaged orbits to correspond to\n",
        "  kappa = 20\n",
        "\n",
        "  # Run phase estimates and phase average \n",
        "  badInds = []\n",
        "  for a in range(lim): #for length of all the trials \n",
        "    print(['processing trial: ' + str(a)]) \n",
        "    dats=[] # reset variables after each trial - kinematics\n",
        "    dats2 = [] # reset the HC params\n",
        "    allsigs = np.empty(shape=[numSegments,])\n",
        "    all_phase_var = np.empty(shape=[numSegments,]) \n",
        "    PhaseAvgPCs = np.empty(shape=[numSegments])\n",
        "    all_cyclephase = []\n",
        "    raw = HC_CellArray[a][:,0:3].T # Only use 1st 3 PCs (of HCs),\n",
        "    raw2 = HC_CellArray[a][:,0:L].T  # extract all the HC data\n",
        "    for b in range(6): #Shai's code does duplication- works better than without duplicating data\n",
        "        dats.append(raw-raw.mean(axis=1)[:,np.newaxis]) #center the HCs data for phase estimation -- this centers each one separately\n",
        "        dats2.append(raw2-raw2.mean(axis=1)[:,np.newaxis])\n",
        "    rHC = raw2 #uncentered \n",
        "\n",
        "    # Run Phaser ONLY if the self-driven prediction doesn't converge to a trivial result (this breaks Phaser)\n",
        "    if a < 1000: # np.std(dats[0][0,:1500]) / np.std(dats[0][0,2250:]) < 1.5: # Only run phaser is variance of the end of the self-driven prediction is greater than 1/2 that of externally-driven prediction\n",
        "      phr = phaser.Phaser(dats) # use centered data from 1st 3 PCs\n",
        "      phi = [ phr.phaserEval( d ) for d in dats ] # extract phase\n",
        "      phi2  = (phi[0].T % (2*pi)); # Take modulo s.t. phase ranges from [0,2*pi]\n",
        "      \n",
        "      # Split into externally and self-driven sets\n",
        "      rHCExt = rHC[:,0:ext_drive]\n",
        "      rHCSelf = rHC[:,ext_drive:]\n",
        "      phi2Ext = phi2[0:ext_drive,0]\n",
        "      phi2Self = phi2[ext_drive:,0]\n",
        "\n",
        "      # find average orbits of ext and self-driven sigs separately\n",
        "      avgOrbitsExt = np.zeros((numSegments,L)) #initialize avg orbits\n",
        "      avgOrbitsSelf = np.zeros((numSegments,L)) #initialize avg orbits\n",
        "      phasesExt = np.reshape(phi2Ext,[ext_drive,]) # Extract phase (external drive)\n",
        "      phasesSelf = np.reshape(phi2Self,[ext_drive,]) # Self-drive\n",
        "\n",
        "      for c in range(numSegments): #number of points in final average orbit/phase averaging\n",
        "        # Externally-driven\n",
        "          vonMiesCurrent = vonMies(phasesExt,phaseVals[c],kappa) # for each value in the num of phase points calculate current\n",
        "          sumVal = np.sum(vonMiesCurrent)\n",
        "          for d in range(L):  # Average each feature\n",
        "              data = np.reshape(rHCExt[d,:],[ext_drive,1])\n",
        "              avgOrbitsExt[c,d] = np.sum(data.T*vonMiesCurrent)/sumVal # phase point (row), feature of PC (column) -- this is generating a value for the 1st phase point of each feature\n",
        "        # Self-driven\n",
        "          vonMiesCurrent = vonMies(phasesSelf,phaseVals[c],kappa) # for each value in the num of phase points calculate current\n",
        "          sumVal = np.sum(vonMiesCurrent)\n",
        "          for d in range(L):  # Average each feature\n",
        "              data = np.reshape(rHCSelf[d,:],[ext_drive,1])\n",
        "              avgOrbitsSelf[c,d] = np.sum(data.T*vonMiesCurrent)/sumVal # phase point (row), feature of PC (column) -- this is generating a value for the 1st phase point of each feature        \n",
        "\n",
        "      # Store phase-averaged results and phase shift the to PC1 max = zero phase\n",
        "      # Use the same phase-shift for both ext- and self-driven signatures\n",
        "      # Externally driven #######################\n",
        "      PhaseAveragedPCsExt[a] = avgOrbitsExt.T # transform to match overall saving structure as before\n",
        "      phi2Ext = phi2Ext.reshape(trialsamp,)\n",
        "      Phase_VariablesExt[a] = phi2Ext # store phase variables for all cycles/features - may need to phase shift these \n",
        "      # Phase Shift - This aligns the Gait signatures in phase\n",
        "      PC1_maxloc = np.argmax(PhaseAveragedPCsExt[a][0]) # only PC1 value (1st max if repeated) - ONLY BASED ON EXT-DRIVEN\n",
        "      Data2Shift = PhaseAveragedPCsExt[a]\n",
        "      NewP = np.roll(Data2Shift, -PC1_maxloc,axis=1) #shift back to orgin\n",
        "      PC_ShiftsExt[a] = PC1_maxloc\n",
        "      PhaseAveragedPCs_shiftExt[a] = NewP\n",
        "\n",
        "      # Self driven #######################\n",
        "      PhaseAveragedPCsSelf[a] = avgOrbitsSelf.T # transform to match overall saving structure as before\n",
        "      phi2Self = phi2Self.reshape(trialsamp,)\n",
        "      Phase_VariablesSelf[a] = phi2Self # store phase variables for all cycles/features - may need to phase shift these \n",
        "      # Phase Shift\n",
        "      Data2Shift = PhaseAveragedPCsSelf[a]\n",
        "      NewP = np.roll(Data2Shift, -PC1_maxloc,axis=1) #shift back to orgin\n",
        "      PC_ShiftsSelf[a] = PC1_maxloc\n",
        "      PhaseAveragedPCs_shiftSelf[a] = NewP\n",
        "\n",
        "      # Compute R2\n",
        "      SSR = np.sum( (np.reshape(PhaseAveragedPCs_shiftExt[a],(-1,1)) - np.reshape(PhaseAveragedPCs_shiftSelf[a], (-1,1) ) )**2 )\n",
        "      SST = np.sum( (np.reshape(PhaseAveragedPCs_shiftExt[a],(-1,1)) - np.mean(np.reshape(PhaseAveragedPCs_shiftExt[a], (-1,1) )) )**2 )\n",
        "      R2[a,0] = 1 - SSR/SST\n",
        "      if R2[a,0] > 0.99:\n",
        "        R2[a,0] = np.nan\n",
        "\n",
        "    else: \n",
        "      print('Trial ' + str(a) + ' is junk.')\n",
        "      badInds.append(a)\n",
        "\n",
        "    # Plot phase-averaged PCs\n",
        "    plt.subplot(12,6,a+1)\n",
        "    print(np.shape(PhaseAveragedPCs_shiftSelf[a]))\n",
        "    plt.plot(PhaseAveragedPCs_shiftExt[a][0,:], label = 'Externally-driven')\n",
        "    plt.plot(PhaseAveragedPCs_shiftSelf[a][0,:], label = 'Self-driven')\n",
        "    if a > 65:\n",
        "      plt.xlabel('Percent gait phase')\n",
        "    if (a % 6) == 0:\n",
        "      plt.ylabel('PC 1 of gait signature')\n",
        "\n",
        "    plt.title('Trial ' + str(a) + ' | R2 = ' + str(np.round(R2[a,0], 2))  )\n",
        "\n",
        "  # Save gait sig alignment\n",
        "  scipy.io.savemat(savepath + mod_name + '_LongTimeAligment.mat',{'R2': R2})\n",
        "\n",
        "\n",
        "  plt.savefig(savepath + mod_name + '_PC1_predictions.png', dpi = 150)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gPLTJgVMt2-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}