{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bermanlabemory/gait_signatures/blob/main/Gait_Signatures_Script_7_Save_Hyperparameter_Results_Single_Folder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This script saves hyperaparameter results from all models in a single folder. \n",
        "\n",
        "Otherwise, you have to scour Google Drive folders (if doing this online) to manually pull each file.\n",
        "____________\n",
        "____________\n",
        "**NOTE**: This file was ONLY used to select model hyperparameters and generate supplementary figures.\n",
        "\n",
        "First, Scripts 1-6 must be run to generate the models that are analyzed here.\n",
        "____________\n",
        "____________\n",
        "**Steps:** \n",
        "1. Load training and validation loss for each model hyperparameter combination\n",
        "  1. Concatenate all training and validation loss values into a single file, along with model specs, and save \n",
        "1. Load short/long-time gait signature alignment results for each model hyperparameter combination\n",
        "  1. Concatenate all training and validation loss values into a single file, along with model specs, and save \n",
        "\n",
        "____________\n",
        "The  gait signature alignment results are stored in a [subject x intialization x model] array.\n",
        "____________\n",
        "\n",
        "**Created by**: Michael C. Rosenberg\n",
        "\n",
        "**Date**: 11/11/22"
      ],
      "metadata": {
        "id": "onLC9Msr6_RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 0**: Mount (connect to) your google drive folder where you want to save the simulation results and model parameters."
      ],
      "metadata": {
        "id": "kLBAyLXe7udu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "Iuzkec-P1z7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fe7bb2-67f9-48ad-de6a-40de8fdbdcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check python version \n",
        "from platform import python_version\n",
        "\n",
        "print(python_version())\n",
        "\n",
        "# check tensorflow version\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# GPU/ram\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "TfiYeSIR702R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3822a459-d47e-4e39-a870-1c1bb448cd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.16\n",
            "2.9.2\n",
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**: Import necessary packages and functions to develop model"
      ],
      "metadata": {
        "id": "WDY0A5Qa77-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# from keras.layers.recurrent import LSTM # Deprecated :(\n",
        "from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.model_selection as model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import keras as k\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import copy\n",
        "import scipy.io\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "from scipy import interpolate\n",
        "from scipy.special import iv\n",
        "from numpy import sin,cos,pi,array,linspace,cumsum,asarray,dot,ones\n",
        "from pylab import plot, legend, axis, show, randint, randn, std,lstsq\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "import csv\n",
        "import os\n",
        "from tqdm import tqdm "
      ],
      "metadata": {
        "id": "Sjnyu-Hi70_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions\n",
        "%cd /content/drive/My Drive/NeuromechanicsLab/GaitSignatures/\n",
        "\n",
        "# Ensure fourierseries.py is in the pathway\n",
        "!ls -l fourierseries.py\n",
        "\n",
        "import fourierseries\n",
        "import util\n",
        "import phaser\n",
        "import dataloader\n",
        "# Preprocess data for a single subject - to be send to modeling frameworks\n",
        "def find_phase(k):\n",
        "    \"\"\"\n",
        "    Detrend and compute the phase estimate using Phaser\n",
        "    INPUT:\n",
        "      k -- dataframe\n",
        "    OUTPUT:\n",
        "      k -- dataframe\n",
        "    \"\"\"\n",
        "    #l = ['hip_flexion_l','hip_flexion_r'] # Phase variables = hip flexion angles\n",
        "    y = np.array(k)\n",
        "    print(y.shape)\n",
        "    y = util.detrend(y.T).T\n",
        "    print(y.shape)\n",
        "    phsr = phaser.Phaser(y=y)\n",
        "    k[:] = phsr.phaserEval(y)[0,:]\n",
        "    return k\n",
        "\n",
        "# Interpolation function\n",
        "def vonMies(t,t_0, b):\n",
        "    out = np.exp(b*np.cos(t-t_0))/(2*pi*iv(0, b))\n",
        "    return out"
      ],
      "metadata": {
        "id": "956M3pr8_56k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10fec4c-4d45-450d-ab5a-eed72d8495df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/NeuromechanicsLab/GaitSignatures\n",
            "-rw------- 1 root root 7259 May 29  2020 fourierseries.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**: Load module in Google Drive"
      ],
      "metadata": {
        "id": "g4J5BZIg8EYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to save the models and read data from\n",
        "path = '/content/drive/My Drive/NeuromechanicsLab/GaitSignatures/'\n",
        "\n",
        "# Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,path)"
      ],
      "metadata": {
        "id": "8PRNNDCI71CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**: Load in data and specify variables/parameters"
      ],
      "metadata": {
        "id": "DAhc9QkW8ONR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-changing variables \n",
        "newSavePath = '/content/drive/My Drive/SETYOURFILEPATH/GaitSignatures/00_HyperparamResults/' # Path where you want to save your results\n",
        "\n",
        "# number of trials in dataset \n",
        "trialnum = 72 # 72 total trials\n",
        "\n",
        "# number of samples in each trial\n",
        "trialsamp = 1500\n",
        "\n",
        "# number of features collected per trial\n",
        "feats = 6\n",
        "\n",
        "#Batch size - same as the number of traintrials\n",
        "batch_size = trialnum\n",
        "\n",
        "# Number of Layers\n",
        "numlayers = 1\n",
        "\n",
        "# Choose the number of iterations to train the model- if this script has been run previously enter a value greater than was \n",
        "# inputted before and rerun the script. \n",
        "finalepoch = 10000\n",
        "\n",
        "# load the input data/kinematics\n",
        "datafilepath = path + 'PareticvsNonP_RNNData.csv' #input data\n",
        "all_csvnp = np.loadtxt(datafilepath,delimiter=',').T\n",
        "\n",
        "# reshape all the input data into a tensor\n",
        "all_inputdata_s = all_csvnp.reshape(trialnum,trialsamp,feats) \n",
        "csvnp = all_inputdata_s\n",
        "print('original input data shape is: '+ str(all_csvnp.shape ))\n",
        "print('input data reshaped is: '+ str(all_inputdata_s.shape))"
      ],
      "metadata": {
        "id": "fXEEUgWB8Mk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fab924-ad59-4e0e-cc0d-d7496e89a1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original input data shape is: (108000, 6)\n",
            "input data reshaped is: (72, 1500, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training vs validation loss"
      ],
      "metadata": {
        "id": "wgJDNFn2NG-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a list of models and corresponding parameters to test \n",
        "test_model_nodes = [128, 256,512,1024] \n",
        "seqs = [249,499,749] #lookback parameter\n",
        "\n",
        "test_model_nodes = [ 512] \n",
        "seqs = [499] #lookback parameter\n",
        "\n",
        "# run multiple model architechtures many times to test stability of cost function outputs\n",
        "runs = 1 # Number of times to train recurrent neural network (RNN) models, starting from random initial conditions. We used this for hyperparameter selection\n",
        "test_model_seq = np.repeat(seqs, runs) # Specify each model's hyperparameters\n",
        "\n",
        "count = np.arange(runs)\n",
        "\n",
        "All_nodes = np.empty([0,1], dtype='int')\n",
        "All_seq = np.empty([0,1],dtype='int')\n",
        "All_valseg = np.empty([0,1],dtype='int')\n",
        "All_trainseg = np.empty([0,1],dtype='int')\n",
        "All_modelname = []\n",
        "All_mod_name = []\n",
        "count = np.empty([0,1],dtype='int'); #initialize model run -- this serves as the model run ID number\n",
        "ct = 0\n",
        "for a in test_model_nodes:\n",
        "  for b in test_model_seq:\n",
        "    count = np.append(count,  ct + 1 )\n",
        "    #if statement for valseg, trainseg based on sequence length\n",
        "    if int(b) == 249:\n",
        "      trainseg = 4\n",
        "      valseg = 2\n",
        "    elif int(b) == 499: \n",
        "      trainseg = 2\n",
        "      valseg = 1\n",
        "    elif int(b) == 749:\n",
        "      trainseg = 1\n",
        "      valseg = 1\n",
        "\n",
        "    # Store resulting model structures and training plans\n",
        "    All_nodes = np.append(All_nodes, a) \n",
        "    All_seq = np.append(All_seq, int(b))\n",
        "    All_valseg = np.append(All_valseg, valseg)\n",
        "    All_trainseg = np.append(All_trainseg, trainseg)\n",
        "    All_modelname = np.append(All_modelname, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) + '/' )\n",
        "    All_mod_name = np.append(All_mod_name, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) )\n",
        "\n",
        "    if ct+1 < runs:\n",
        "      ct += 1\n",
        "    else:\n",
        "      ct = 0\n",
        "print(All_mod_name)"
      ],
      "metadata": {
        "id": "SUjh8nTkNI5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load training and validation loss**"
      ],
      "metadata": {
        "id": "qLlw-EDwOUHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TR = np.empty([0,1], dtype='int') # Val Loss [subject x runs x models]\n",
        "VAL = np.empty([0,1], dtype='int') # Val Loss [subject x runs x models]\n",
        "ND = np.empty([0,1], dtype='int') # nodes\n",
        "LB = np.empty([0,1], dtype='int') # Lookback\n",
        "\n",
        "for j in range(len(All_mod_name)):\n",
        "  if j != 47:\n",
        "    # extract path to store each model and generated data\n",
        "    savepath = path + All_modelname[j]\n",
        "    mod_name = All_mod_name[j]\n",
        "\n",
        "    #print(savepath + mod_name + '_MIN_train_loss.npy')\n",
        "    if os.path.exists(savepath + mod_name + '_MIN_train_loss.npy'):\n",
        "      tempTr= np.load(savepath + mod_name + '_MIN_train_loss.npy')\n",
        "      tempVal= np.load(savepath + mod_name + '_MIN_val_loss.npy')\n",
        "      TR = np.append(TR,tempTr)\n",
        "      VAL = np.append(VAL,tempVal)\n",
        "      LB = np.append(LB, int(mod_name[mod_name.find('LB')+3:mod_name.find('LB')+6]))\n",
        "      ND = np.append(ND, int(mod_name[mod_name.find('IT_')+3:mod_name.find('_LB')]))\n",
        "\n",
        "scipy.io.savemat(newSavePath + 'trainValLoss.mat',{'TR':TR, 'VAL':VAL, 'numnode':ND, 'numLB':LB})"
      ],
      "metadata": {
        "id": "X4i0UhhwOPFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gait Signature alignment"
      ],
      "metadata": {
        "id": "q_A7pitQ8XS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a list of models and corresponding parameters to test \n",
        "test_model_nodes = [128, 256,512,1024] \n",
        "seqs = [249,499,749] #lookback parameter\n",
        "\n",
        "test_model_nodes = [ 512] \n",
        "seqs = [499] #lookback parameter\n",
        "\n",
        "# run multiple model architechtures many times to test stability of cost function outputs\n",
        "runs = 1 # Number of times to train recurrent neural network (RNN) models, starting from random initial conditions. We used this for hyperparameter selection\n",
        "test_model_seq = np.repeat(seqs, runs) # Specify each model's hyperparameters\n",
        "\n",
        "count = np.arange(runs)\n",
        "\n",
        "All_nodes = np.empty([0,1], dtype='int')\n",
        "All_seq = np.empty([0,1],dtype='int')\n",
        "All_valseg = np.empty([0,1],dtype='int')\n",
        "All_trainseg = np.empty([0,1],dtype='int')\n",
        "All_modelname = []\n",
        "All_mod_name = []\n",
        "count = np.empty([0,1],dtype='int'); #initialize model run -- this serves as the model run ID number\n",
        "ct = 0\n",
        "for a in test_model_nodes:\n",
        "  for b in test_model_seq:\n",
        "    count = np.append(count,  ct + 1 )\n",
        "    #if statement for valseg, trainseg based on sequence length\n",
        "    if int(b) == 249:\n",
        "      trainseg = 4\n",
        "      valseg = 2\n",
        "    elif int(b) == 499: \n",
        "      trainseg = 2\n",
        "      valseg = 1\n",
        "    elif int(b) == 749:\n",
        "      trainseg = 1\n",
        "      valseg = 1\n",
        "\n",
        "    # Store resulting model structures and training plans\n",
        "    All_nodes = np.append(All_nodes, a) \n",
        "    All_seq = np.append(All_seq, int(b))\n",
        "    All_valseg = np.append(All_valseg, valseg)\n",
        "    All_trainseg = np.append(All_trainseg, trainseg)\n",
        "    All_modelname = np.append(All_modelname, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) + '/' )\n",
        "    All_mod_name = np.append(All_mod_name, 'UNIT_' + str(a) + '_LB_' + str(b) + '_run_' + str(count[-1]) )\n",
        "\n",
        "    if ct+1 < runs:\n",
        "      ct += 1\n",
        "    else:\n",
        "      ct = 0\n",
        "print(All_mod_name)"
      ],
      "metadata": {
        "id": "vAN8l21V8ZeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f29ce8e-d269-4686-ac7a-c31006e90d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "['UNIT_256_LB_499_run_1' 'UNIT_256_LB_499_run_2' 'UNIT_256_LB_499_run_3'\n",
            " 'UNIT_256_LB_499_run_4' 'UNIT_256_LB_499_run_5' 'UNIT_256_LB_499_run_6'\n",
            " 'UNIT_256_LB_499_run_7' 'UNIT_256_LB_499_run_8' 'UNIT_256_LB_499_run_9'\n",
            " 'UNIT_256_LB_499_run_10' 'UNIT_256_LB_749_run_1' 'UNIT_256_LB_749_run_2'\n",
            " 'UNIT_256_LB_749_run_3' 'UNIT_256_LB_749_run_4' 'UNIT_256_LB_749_run_5'\n",
            " 'UNIT_256_LB_749_run_6' 'UNIT_256_LB_749_run_7' 'UNIT_256_LB_749_run_8'\n",
            " 'UNIT_256_LB_749_run_9' 'UNIT_256_LB_749_run_10' 'UNIT_512_LB_499_run_1'\n",
            " 'UNIT_512_LB_499_run_2' 'UNIT_512_LB_499_run_3' 'UNIT_512_LB_499_run_4'\n",
            " 'UNIT_512_LB_499_run_5' 'UNIT_512_LB_499_run_6' 'UNIT_512_LB_499_run_7'\n",
            " 'UNIT_512_LB_499_run_8' 'UNIT_512_LB_499_run_9' 'UNIT_512_LB_499_run_10']\n",
            "/content/drive/My Drive/NeuromechanicsLab/GaitSignatures/UNIT_256_LB_499_run_1/UNIT_256_LB_499_run_1_bestwhole.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load short & long-time gait signature alignment**"
      ],
      "metadata": {
        "id": "rVW0Xhdi8nP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ST = np.zeros((trialnum,runs,3)) # [subject x runs x models]\n",
        "R2_ST = np.zeros((trialnum,runs,3)) # [subject x runs x models]\n",
        "\n",
        "for j in range(len(All_mod_name)):\n",
        "  # extract path to store each model and generated data\n",
        "  savepath = path + All_modelname[j]\n",
        "  mod_name = All_mod_name[j]\n",
        "\n",
        "  # Load results\n",
        "  temp = scipy.io.loadmat(savepath + mod_name + '_shortTimeAlignment.mat')\n",
        "  # Load Euclidean distance alignment to identify bogus trials\n",
        "  EuclidAlignment = temp['EuclideanAlignment']\n",
        "  ST[:,j % 10, int(np.ceil((j+1)/10))-1] = ShortTimeAlignment[:,0]\n",
        "\n",
        "  # Extract alignment results\n",
        "  R2 = temp['R2']\n",
        "  R2_ST[:,j % 10, int(np.ceil((j+1)/10))-1] = R2[:,0]\n",
        "\n",
        "# Remove any short-time sigs where clean cycles were not identified\n",
        "badInds, temp =  np.where(np.any(ST == 0, axis = 1)) \n",
        "ST = np.delete(ST, np.unique(badInds), 0)\n",
        "R2_ST = np.delete(R2_ST, np.unique(badInds), 0)\n",
        "\n",
        "\n",
        "# LONG TIME ####################\n",
        "LT = np.zeros((trialnum,runs,3)) # [subject x runs x models]\n",
        "R2_LT = np.zeros((trialnum,runs,3)) # [subject x runs x models]\n",
        "print(np.shape(LT))\n",
        "\n",
        "for j in range(len(All_mod_name)):\n",
        "  savepath = path + All_modelname[j]\n",
        "  mod_name = All_mod_name[j]\n",
        "\n",
        "  # Load long-time alignment results\n",
        "  temp = scipy.io.loadmat(savepath + mod_name + '_LongTimeAligment.mat')\n",
        "  # Extract R-squared\n",
        "  R2 = temp['R2']\n",
        "  R2_LT[:,j % 10, int(np.ceil((j+1)/10))-1] = R2[:,0]\n",
        "\n",
        "# Remove bad trials (same trials as short-time alignment)\n",
        "R2_LT = np.delete(R2_LT, np.unique(badInds), 0)\n",
        "\n",
        "# Save\n",
        "scipy.io.savemat(newSavePath + 'gaitSigAlignment.mat',{'R2_ST':R2_ST, 'R2_LT':R2_LT})"
      ],
      "metadata": {
        "id": "E2UNrdO2Ziw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad96fdf1-fe61-4be4-cdaf-e9367e1ffa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(72, 10, 3)\n"
          ]
        }
      ]
    }
  ]
}